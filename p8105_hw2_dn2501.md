Homework \#2
================
David Nemirovsky
9/30/20

``` r
library(tidyverse)
library(readxl)
```

## Problem 1

Read the Mr. Trashwheel dataset:

``` r
trashwheel_df = 
  read_xlsx(
    "./hw2_data/Trash-Wheel-Collection-Totals-8-6-19.xlsx",
    sheet = "Mr. Trash Wheel",
    range = cell_cols("A:N")) %>% 
  janitor::clean_names() %>% 
  drop_na(dumpster) %>% 
  mutate(
    sports_balls = round(sports_balls),
    sports_balls = as.integer(sports_balls))
```

Read the precipitation data:

``` r
precip_2018 = 
  read_excel(
    "./hw2_data/Trash-Wheel-Collection-Totals-8-6-19.xlsx", 
    sheet = "2018 Precipitation",
    skip = 1) %>% 
  janitor::clean_names() %>% 
  drop_na(month) %>% 
  mutate(year = 2018) %>% 
  relocate(year)

precip_2017 = 
  read_excel(
    "./hw2_data/Trash-Wheel-Collection-Totals-8-6-19.xlsx", 
    sheet = "2017 Precipitation",
    skip = 1) %>% 
  janitor::clean_names() %>% 
  drop_na(month) %>% 
  mutate(year = 2017) %>% 
  relocate(year)
```

Now, combine annual precipitation:

``` r
month_df = 
  tibble(
    month = 1:12,
    month_name = month.name)

precip_df = 
  bind_rows(precip_2018, precip_2017)

precip_df =
left_join(precip_df, month_df, by = "month")
```

This dataset contains information from the Mr. Trashwheel trash
collector located in Baltimore, Maryland. Mr. Trashwheel collects trash
as it enters the inner harbor and stores it in a dumpster. This dataset
contains information on year, month, and trash collected, including some
specific types of trash. There are a total of 344 rows in our data set.
Additional data sheets include monthly precipitation data. The total
precipitation in 2018 was 70.33 inches. The median number of sports
balls found in a dumpster in 2017 was 8.

## Problem 2

Read in, clean names, and select the appropriate NYC Transit dataset:

``` r
transit_df = 
  read_csv("./hw2_data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>% 
  janitor::clean_names() %>% 
  select(line:entry, vending, ada) %>% 
  unite(route, route1:route11, sep = ", ", na.rm = TRUE) %>% 
  relocate(station_name, route) %>% 
  arrange(station_name, route) %>% 
  mutate(entry = recode(entry, "YES" = TRUE, "NO" = FALSE))
```

This dataset contains information about the entrances of all of the NYC
subway stations. Some key variables are subway lines, routes, types of
entry, presence of MetroCard vending machines, and whether or not that
station entrance is ADA compliant. A variety of functions were used in
order to tidy this data, including `clean_names` from the `janitor`
package to rename the variables in snake-case. The functions`select`,
`unite`, `relocate`, `arrange`, and `mutate` (from the `dplyr` package
in `tidyverse`) were used to choose the appropriate data to study,
collapse the “route\#” variables into a single variable indicating train
routes, re-position and rearrange the columns into a more appealing
order, and convert the “entry” character variable to a logical variable,
respectively. The dataset is composed of 1868 rows and 9 columns.

#### Answers to Questions about Data:

1.  There are 456 distinct stations.
2.  Of these, only 79 stations are ADA compliant.
3.  Approximately 37.7% of station entrances/exits without vending allow
    entrance.

Now, reformat data to create separate variables for “route number” and
“route name”:

``` r
transit_reformed_df = 
  read_csv("./hw2_data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>% 
  janitor::clean_names() %>% 
  select(line:entry, vending, ada) %>% 
  mutate(across(route8:route11, as.character)) %>% 
  pivot_longer(route1:route11,
    names_to = "route_number",
    names_prefix = "route",
    values_to = "route_name",
    values_drop_na = TRUE) %>% 
  relocate(station_name, route_number, route_name) %>% 
  arrange(station_name, route_number, route_name) %>% 
  mutate(entry = recode(entry, "YES" = TRUE, "NO" = FALSE))
```

There are 56 distinct stations that serve the A train. Of these, only 16
stations are ADA compliant.

## Problem 3

Read in and clean the politician dataset:

``` r
month_df = 
  tibble(
    month = 1:12,
    month_name = month.name)

pols_df = 
  read_csv("./hw2_data/fivethirtyeight_datasets/pols-month.csv") %>% 
  janitor::clean_names() %>% 
  separate(mon, into = c("year", "month", "day"), sep = "-") %>% 
  mutate(across(year:day, as.integer)) %>% 
  left_join(month_df, by = "month") %>% 
  relocate(year:day, prez_gop, prez_dem) %>% 
  pivot_longer(
    prez_gop:prez_dem,
    names_to = "president",
    names_prefix = "prez_",
    values_to = "party") %>% 
  mutate(party = na_if(party, 0)) %>% 
  drop_na(party) %>% 
  relocate(year, month_name, president) %>% 
  select(year:rep_dem, -month, -day) %>% 
  rename(month = month_name)
```

Read in and clean S\&P 500 dataset:

``` r
snp_df = 
  read_csv("./hw2_data/fivethirtyeight_datasets/snp.csv") %>% 
  janitor::clean_names() %>% 
  separate(date, into = c("month", "day", "year"), sep = "/") %>% 
  mutate(across(month:year, as.integer)) %>% 
  arrange(year, month) %>% 
  left_join(month_df, by = "month") %>%
  select(year:month_name) %>% 
  relocate(year, month_name) %>% 
  rename(month = month_name)
```

Read in and clean unemployment dataset:

``` r
month_short_df = 
  tibble(
    month_short = c("jan", "feb", "mar", "apr", "may", "jun", "jul", "aug", "sep", "oct", "nov", "dec"),
    month = month.name)

unemp_df = 
  read_csv("./hw2_data/fivethirtyeight_datasets/unemployment.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    jan:dec,
    names_to = "month_short",
    values_to = "unemployment") %>% 
  left_join(month_short_df, by = "month_short") %>% 
  select(year, month, unemployment)
```

Join all 3 datasets:

``` r
merged_df = 
  left_join(pols_df, snp_df, by = c("year", "month")) %>% 
  left_join(unemp_df, by = c("year", "month")) %>% 
  drop_na(close, unemployment) %>% 
  relocate(year:president, close, unemployment) %>% 
  rename(snp_close = close)
```

The dataset `pols_df` contained information about the political parties
of politicians in the Senate and House of Representatives, as well as
the party of the President and governors from January 1947 to June 2015.
The `snp_df` dataset included the price at which the S\&P 500 closed at
the beginning of each month from January 1950 to July 2015. The
`unemp_df` dataset included national unemployment percentages from
January 1948 to December 2015. The merged dataset, `merged_df`, joins
the 3 datasets above into a single one and consists of 786 rows and 11
columns. This dataset includes the key variables described above, but
from January 1950 to June 2015. This dataset can be used to assess how
the dominance of a political party in the federal government can
influence national economic prosperity.
